{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSS AND ACCURACY PLOT\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    fig.savefig('plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.legacy.preprocessing.image.ImageDataGenerator object at 0x00000193413189B0>\n",
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data gfenerators\n",
    "\n",
    "\n",
    "train_dir = 'C:/Users/athen/OneDrive/Documents/Mini project 2/archive (1)/images/images/train'\n",
    "val_dir = 'C:/Users/athen/OneDrive/Documents/Mini project 2/archive (1)/images/images/test'\n",
    "\n",
    "num_train = 28709\n",
    "num_val = 7178\n",
    "batch_size = 64\n",
    "num_epoch = 2\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "print(datagen)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=batch_size,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, storage\n",
    "import requests\n",
    "\n",
    "service_account_key = 'C:/Users/athen/OneDrive/Documents/Mini project 2/mini-project-d9780-firebase-adminsdk-excc6-1f7073b6d8.json'\n",
    "\n",
    "# Check if Firebase app is already initialized\n",
    "try:\n",
    "    firebase_admin.get_app()\n",
    "except ValueError:\n",
    "    # Initialize Firebase app with credentials and storage bucket\n",
    "    cred = credentials.Certificate(service_account_key)\n",
    "    firebase_admin.initialize_app(cred, {'storageBucket': 'mini-project-d9780.appspot.com'})\n",
    "bucket = storage.bucket()\n",
    "\n",
    "from google.cloud import storage as gcs_storage\n",
    "\n",
    "def generate_signed_url(blob_name):\n",
    "    expiration_time = 100000  # URL expires in 1 hour (3600 seconds)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    signed_url = blob.generate_signed_url(expiration=expiration_time)\n",
    "    return signed_url\n",
    "\n",
    "blobs = bucket.list_blobs()\n",
    "video_urls = [generate_signed_url(blob.name) for blob in blobs if blob.name.endswith('.mp4')]\n",
    "print(\"Generated URLs:\")\n",
    "for url in video_urls:\n",
    "    print(url)\n",
    "\n",
    "def stream_video_from_url(url, detector, display=False, save_path=\"frames\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Could not open video from URL: {url}\")\n",
    "        return []\n",
    "\n",
    "    byte_stream = bytes()\n",
    "    frame_count = 0\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            byte_stream += chunk\n",
    "\n",
    "            start_idx = byte_stream.find(b'\\x00\\x00\\x01\\xba')\n",
    "            end_idx = byte_stream.find(b'\\x00\\x00\\x01\\xb9')\n",
    "\n",
    "            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "                frame_data = byte_stream[start_idx:end_idx]\n",
    "                byte_stream = byte_stream[end_idx + 4:]\n",
    "\n",
    "                np_array = np.frombuffer(frame_data, np.uint8)\n",
    "                frame = cv2.imdecode(np_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "                if frame is not None:\n",
    "                    result = detector.detect_emotions(frame)\n",
    "                    results.append((frame_count, result))\n",
    "                    print(f\"Frame {frame_count}: {result}\")\n",
    "\n",
    "                    if display:\n",
    "                        for emotion in result:\n",
    "                            cv2.putText(frame, str(emotion), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                        frame_filename = os.path.join(save_path, f\"frame_{frame_count:04d}.jpg\")\n",
    "                        cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "                    frame_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        print(\"Analysis complete.\")\n",
    "        return results\n",
    "\n",
    "blobs = bucket.list_blobs()\n",
    "video_urls = [generate_signed_url(blob.name) for blob in blobs if blob.name.endswith('.mp4')]\n",
    "\n",
    "\n",
    "class EmotionDetector:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    \n",
    "    def predict(self, frame):\n",
    "        \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "        frame = cv2.resize(frame, (48, 48))   \n",
    "        frame = frame / 255.0 \n",
    "        frame = frame.reshape(1, 48, 48, 1) \n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(frame)\n",
    "        return predictions\n",
    "\n",
    "    def detect_emotions(self,frame):\n",
    "        predictions = self.predict(frame)\n",
    "        \n",
    "        # Convert predictions to emotion labels\n",
    "        # This is a dummy implementation, replace with your actual logic\n",
    "        emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "        detected_emotions = [emotions[pred.argmax()] for pred in predictions]\n",
    "\n",
    "        return detected_emotions \n",
    "\n",
    "def process_videos_from_firebase(bucket_name, model, detector):\n",
    "    bucket = storage.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.mp4'):\n",
    "            video_url = generate_signed_url(blob.name)\n",
    "            print(f\"Processing video from URL: {video_url}\")\n",
    "            stream_video_from_url(video_url, detector, display=True)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = 'emotion_detection_model.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "emotion_detector = EmotionDetector(model)\n",
    "\n",
    "# Stream and analyze each video from Firebase Storage\n",
    "for video_url in video_urls:\n",
    "    print(f\"Processing video from URL: {video_url}\")\n",
    "    stream_video_from_url(video_url, emotion_detector, display=True)\n",
    "\n",
    "class Video:\n",
    "    def __init__(self, video_blob):\n",
    "        self.path =video_blob\n",
    "        self.capture = cv2.VideoCapture(video_blob)\n",
    "        if not self.capture.isOpened():\n",
    "            print(\"Error: Could not open video.\")\n",
    "\n",
    "    \n",
    "    def analyze(self, detector, display=False, save_path=\"frames\"):\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        frame_count = 0\n",
    "        results = []\n",
    "        while True:\n",
    "            ret, frame = self.capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "    \n",
    "            # Perform emotion detection on the frame\n",
    "            result = detector.detect_emotions(frame) \n",
    "            results.append((frame_count, result))\n",
    "            print(f\"Frame {frame_count}: {result}\")# Fixing the method call\n",
    "    \n",
    "            if display:\n",
    "                for emotion in result:\n",
    "                    cv2.putText(frame, str(emotion), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                frame_filename = os.path.join(save_path, f\"frame_{frame_count:04d}.jpg\")\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "                frame_count += 1\n",
    "\n",
    "        self.capture.release()\n",
    "        print(\"Analysis complete.\") \n",
    "        return results\n",
    "\n",
    "def process_videos_from_firebase(video_urls, emotion_detector):\n",
    "    for video_url in video_urls:\n",
    "        print(f\"Processing video from URL: {video_url}\")\n",
    "        results = stream_video_from_url(video_url, emotion_detector, display=True)\n",
    "        # Optionally, save or process results as needed\n",
    "\n",
    "# Call the function to process videos from Firebase Storage\n",
    "process_videos_from_firebase(video_urls, EmotionDetector(model))\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.save('emotion_detection_model.h5')\n",
    "model = keras.models.load_model(\"emotion_detection_model.h5\")\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "for video_url in video_urls:\n",
    "        print(f\"Processing video from URL: {video_url}\")\n",
    "        stream_video_from_url(video_url, emotion_detector, display=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=num_train // batch_size,\n",
    "    epochs=num_epoch,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_val // batch_size)\n",
    "\n",
    "bucket_name = 'mini-project-d9780.appspot.com'  # Replace with your Firebase Storage bucket name\n",
    "process_videos_from_firebase(bucket_name, EmotionDetector(model))\n",
    "\n",
    "video = Video(video_url)\n",
    "emotion_detector = EmotionDetector(model)\n",
    "results = video.analyze(emotion_detector, display=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results is None:\n",
    "    print(\"Error: analyze method returned None\")\n",
    "else:\n",
    "    # Convert results to pandas DataFrame\n",
    "   data = {\"frame_count\": [], \"happy\": [], \"surprise\": [], \n",
    "        \"angry\": [], \"disgust\": [], \"fear\": [], \"sad\": []}\n",
    "\n",
    "# Populate the DataFrame with results\n",
    "for frame_count, emotions in results:\n",
    "    data[\"frame_count\"].append(frame_count)\n",
    "    data[\"happy\"].append(emotions.count(\"happy\"))\n",
    "    data[\"surprise\"].append(emotions.count(\"surprise\"))\n",
    "    data[\"angry\"].append(emotions.count(\"angry\"))\n",
    "    data[\"disgust\"].append(emotions.count(\"disgust\"))\n",
    "    data[\"fear\"].append(emotions.count(\"fear\"))\n",
    "    data[\"sad\"].append(emotions.count(\"sad\"))\n",
    "    emotions_df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(emotions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person is not interested\n"
     ]
    }
   ],
   "source": [
    "# Predict whether a person show interest in a topic or not\n",
    "\n",
    "positive_emotions = sum(emotions_df.happy) + sum(emotions_df.surprise)\n",
    "negative_emotions = sum(emotions_df.angry) + sum(emotions_df.disgust) + sum(emotions_df.fear) + sum(emotions_df.sad)\n",
    "\n",
    "if positive_emotions > negative_emotions:\n",
    "    print(\"Person is interested\")\n",
    "    update_result(\"Person is satisfied\")\n",
    "elif positive_emotions < negative_emotions:\n",
    "    print(\"Person is not interested\")\n",
    "    update_result(\"Person is not satisfied\")\n",
    "else:\n",
    "    print(\"Person is neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to neon\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "    'host' : 'ep-wild-scene-a1bnv5tq.ap-southeast-1.aws.neon.tech',\n",
    "    'database' : 'miniproject',\n",
    "    'user' : 'athena',\n",
    "    'password' : '0hStGLnb5fDa',\n",
    "    'port': '5432'\n",
    "}\n",
    "engine = create_engine(f'postgresql://{db_params['user']}:{db_params['password']}@{db_params['port']}/{db_params['database']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=db_params['ep-wild-scene-a1bnv5tq.ap-southeast-1.aws.neon.tech'],\n",
    "    database=db_params['miniproject'],\n",
    "    user=db_params['athena'],\n",
    "    password=db_params['0hStGLnb5fDa'],\n",
    "    port=db_params['5432']\n",
    "\n",
    "cursor = conn.cursor()\n",
    "def update_result(result):\n",
    "    cursor.execute('UPDATE uploaded_videos VALUES ()', (result,))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    success,frame=camera.read()\n",
    "    if success:\n",
    "        img=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        \n",
    "        cv2.imshow(\"output\",frame)\n",
    "        k=cv2.waitKey(1)\n",
    "        if k==ord(\"a\"):\n",
    "            camera.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
